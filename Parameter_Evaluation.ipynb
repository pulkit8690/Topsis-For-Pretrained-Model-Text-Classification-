{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPFA7pMtLR1NbHKub4SG9gx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pulkit8690/Topsis-For-Pretrained-Model-Text-Classification-/blob/main/Parameter_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "a4yOWd2A6Rqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef, cohen_kappa_score,log_loss\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "a04GqQHuxLhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    \"Sakil/IMDB_URDUSENTIMENT_MODEL\",\n",
        "    \"mnoukhov/gpt2-imdb-sentiment-classifier\",\n",
        "    \"AdapterHub/bert-base-uncased-pf-imdb\",\n",
        "    \"wrmurray/roberta-base-finetuned-imdb\",\n",
        "    \"kurianbenoy/distilbert-base-uncased-finetuned-sst-2-english-finetuned-imdb\",\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "y_RrEy2cxXP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "id": "JZhJVOg-D_c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('imdb',streaming=True)"
      ],
      "metadata": {
        "id": "9YJnRh1zyXks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [item  for item in dataset['test'] if len(item['text']) <512]\n",
        "len(data)"
      ],
      "metadata": {
        "id": "AweohILuyyXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "ybFpTgGKy05m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_ROWS = 2000"
      ],
      "metadata": {
        "id": "7Eg-koYDy2il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newDf = pd.concat([df[df.label == 0].head(NUM_ROWS //2),df[df.label ==1].head(NUM_ROWS//2)]).sample(frac = 1).reset_index(drop = True)\n"
      ],
      "metadata": {
        "id": "MFKofxcAy8si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newDf.head()"
      ],
      "metadata": {
        "id": "P_4ig7Ony--G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = newDf.text.tolist()\n",
        "labels = newDf.label.tolist()"
      ],
      "metadata": {
        "id": "nT6r6nEuzBv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x = labels)"
      ],
      "metadata": {
        "id": "-sA5_DQCzEk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FORMAT_LABELS ={\n",
        "    'Label_0':0,\n",
        "    \"Label_1\":1,\n",
        "    \"POSITIVE\":1,\n",
        "    \"NEGATIVE\":0,\n",
        "    'neg':0,\n",
        "    'pos':1\n",
        "}"
      ],
      "metadata": {
        "id": "juIIvG8JzLm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model_name):\n",
        "\n",
        "    print('model initialized')\n",
        "    pipe = pipeline(\"text-classification\", model=model_name)\n",
        "    start_time = time.time()\n",
        "    res = pipe(texts)\n",
        "    end_time = time.time()\n",
        "\n",
        "    predicted_labels = list(map(lambda x: FORMAT_LABELS.get(x['label'],0),res))\n",
        "    probs = [ item['score'] for item in res ]\n",
        "    print('Calculate evaluation metrics')\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(labels, predicted_labels)\n",
        "    precision = precision_score(labels, predicted_labels, average=\"weighted\")\n",
        "    recall = recall_score(labels, predicted_labels, average=\"weighted\")\n",
        "    f1 = f1_score(labels, predicted_labels, average=\"weighted\")\n",
        "    roc_auc = roc_auc_score(labels, predicted_labels)\n",
        "    avg_precision = average_precision_score(labels, predicted_labels)\n",
        "    mcc = matthews_corrcoef(labels, predicted_labels)\n",
        "    kappa = cohen_kappa_score(labels, predicted_labels)\n",
        "    logloss = log_loss(labels,probs)\n",
        "\n",
        "    # Calculate training time\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1,\n",
        "        \"ROC-AUC\": roc_auc,\n",
        "        \"Average Precision\": avg_precision,\n",
        "        \"Matthews Correlation Coefficient\": mcc,\n",
        "        \"Cohen's Kappa\": kappa,\n",
        "        \"Time (s)\": training_time,\n",
        "        'Log Loss': logloss,\n",
        "    }"
      ],
      "metadata": {
        "id": "EPP0EHhqzN4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for model_name in models:\n",
        "        try:\n",
        "            result = evaluate_model(model_name)\n",
        "            results.append(result)\n",
        "            print(\"Done\",model_name)\n",
        "        except:\n",
        "            print(model_name)\n",
        "\n",
        "# making a dataframe\n",
        "df = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "V9y5QiPCzP0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "hyxJs1Dozabj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"Inputdata.csv\")"
      ],
      "metadata": {
        "id": "1k1VAr6zzeeP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}